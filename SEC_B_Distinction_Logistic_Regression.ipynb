{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# SEC-B Assignment - Machine Learning\n", "## Logistic Regression using TensorFlow (Distinction Level)\n", "**Name:** Aaryat Khatri  \n", "**Course:** BCA (AI & Data Science)**\n", "---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Role of Weights in a Neuron\n", "Weights determine the importance of input features in a neuron.\n", "\n", "Mathematical Representation:\n", "z = w1x1 + w2x2 + ... + b\n", "\n", "Weights are updated during training using Gradient Descent."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 2. Activation Function\n", "An activation function introduces non-linearity.\n", "\n", "In Logistic Regression, we use the **Sigmoid Function**:\n", "\u03c3(x) = 1 / (1 + e^-x)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 3. Probability Distribution in ML\n", "Logistic Regression assumes the output follows a **Bernoulli Distribution** (0 or 1)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Gradient in Optimization\n", "Gradient is the derivative of the loss function.\n", "\n", "Update Rule:\n", "w = w - learning_rate * gradient"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Practical Implementation - Logistic Regression using TensorFlow"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "import tensorflow as tf\n", "import matplotlib.pyplot as plt\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.datasets import make_classification\n", "from sklearn.preprocessing import StandardScaler\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Create synthetic dataset\n", "X, y = make_classification(n_samples=1000,\n", "                           n_features=2,\n", "                           n_classes=2,\n", "                           random_state=42)\n", "\n", "# Scale features\n", "scaler = StandardScaler()\n", "X = scaler.fit_transform(X)\n", "\n", "# Train-test split\n", "X_train, X_test, y_train, y_test = train_test_split(\n", "    X, y, test_size=0.2, random_state=42)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Build Logistic Regression Model\n", "model = tf.keras.Sequential([\n", "    tf.keras.layers.Dense(1, activation='sigmoid')\n", "])\n", "\n", "model.compile(optimizer='adam',\n", "              loss='binary_crossentropy',\n", "              metrics=['accuracy'])\n", "\n", "history = model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Evaluate Model\n", "loss, accuracy = model.evaluate(X_test, y_test)\n", "print('Test Accuracy:', accuracy)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Visualization - Decision Boundary\n", "plt.figure()\n", "\n", "# Create mesh grid\n", "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n", "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n", "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100),\n", "                     np.linspace(y_min, y_max, 100))\n", "\n", "grid = np.c_[xx.ravel(), yy.ravel()]\n", "preds = model.predict(grid)\n", "preds = preds.reshape(xx.shape)\n", "\n", "plt.contourf(xx, yy, preds, alpha=0.3)\n", "plt.scatter(X[:, 0], X[:, 1], c=y)\n", "plt.title('Logistic Regression Decision Boundary')\n", "plt.show()\n"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.x"}}, "nbformat": 4, "nbformat_minor": 2}